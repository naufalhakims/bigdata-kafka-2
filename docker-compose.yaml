version: '3.8'

networks:
  kafka-net:
    driver: bridge

services:
  zookeeper:
    image: wurstmeister/zookeeper:latest
    container_name: zookeeper
    ports:
      - "2181:2181"
    networks:
      - kafka-net

  kafka:
    image: wurstmeister/kafka:latest
    container_name: kafka
    ports:
      - "9092:9092"
    environment:
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT_INTERNAL://kafka:9093,PLAINTEXT_EXTERNAL://localhost:9092
      KAFKA_LISTENERS: PLAINTEXT_INTERNAL://0.0.0.0:9093,PLAINTEXT_EXTERNAL://0.0.0.0:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT_INTERNAL:PLAINTEXT,PLAINTEXT_EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT_INTERNAL
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_CREATE_TOPICS: "food-data-topic:1:1" # Automatically create the topic
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    depends_on:
      - zookeeper
    networks:
      - kafka-net

  pyspark-app-trainer: # Renamed for clarity
    build:
      context: ./pyspark_app
    container_name: pyspark-app-trainer
    depends_on:
      - kafka # Depends on Kafka mainly to ensure network is up, not direct consumption
    networks:
      - kafka-net
    volumes:
      - ./data:/app/data # Mount local data dir to /app/data in container
    command: >
      spark-submit
      --master local[*]
      /app/spark_model_trainer.py
    # This service will run to completion and then exit.
    # Use `docker-compose up pyspark-app-trainer` to run it.
    # Or add restart: on-failure if you want it to retry.
    # For this task, it's a one-off training run after data is batched.

  api-server:
    build:
      context: ./api_server
    container_name: api-server
    ports:
      - "5001:5000" # Expose API on host port 5001
    depends_on:
      - pyspark-app-trainer # Optional: wait for trainer to finish if running all at once
                           # More practically, you run trainer, then separately start API
    networks:
      - kafka-net # May not be strictly needed if not talking to kafka, but good for consistency
    volumes:
      - ./data/models:/app/models:ro # Mount models read-only
    environment:
      - MODELS_DIR=/app/models
    # restart: always # If you want the API to always be running
